{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip --trusted-host pypi.org --trusted-host files.pythonhosted.org install -r requirements.txt -qq\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "from transformers import *\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score\n",
    "from ast import literal_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 환경설정\n",
    "## 기타설정\n",
    "os.makedirs('tokens', exist_ok=True)\n",
    "os.makedirs('model', exist_ok=True)\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "## 환경변수\n",
    "VER = 0.1\n",
    "MODEL_NAME = 'google/bigbird-roberta-base'\n",
    "CONFIG = {\n",
    "    'model_name': MODEL_NAME,\n",
    "    'max_length': 1024,\n",
    "    'train_batch_size': 4,\n",
    "    'valid_batch_size': 4,\n",
    "    'epochs': 5,\n",
    "    'learning_rates': [2.5e-5, 2.5e-5, 2.5e-6, 2.5e-6, 2.5e-7],\n",
    "    'max_grad_norm': 10,\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "}\n",
    "COMPUTE_VAL_SCORE = True if len(os.listdir('data/test')) <= 5 else False\n",
    "\n",
    "OUTPUT_LABELS = [\n",
    "    'O', 'B-Lead', 'I-Lead', 'B-Position', 'I-Position',\n",
    "    'B-Claim', 'I-Claim', 'B-Counterclaim', 'I-Counterclaim', \n",
    "    'B-Rebuttal', 'I-Rebuttal', 'B-Evidence', 'I-Evidence',\n",
    "    'B-Concluding Statement', 'I-Concluding Statement'\n",
    "]\n",
    "LABELS_TO_IDS = {v: k for k, v in enumerate(OUTPUT_LABELS)}\n",
    "IDS_TO_LEBELS = {k: v for k, v in enumerate(OUTPUT_LABELS)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/bigbird-roberta-base were not used when initializing BigBirdForTokenClassification: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BigBirdForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BigBirdForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BigBirdForTokenClassification were not initialized from the model checkpoint at google/bigbird-roberta-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "## 모델 설정\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, add_prefix_space=True)\n",
    "tokenizer.save_pretrained('model')\n",
    "\n",
    "config_model = AutoConfig.from_pretrained(MODEL_NAME)\n",
    "config_model.num_labels = 15\n",
    "config_model.save_pretrained('model')\n",
    "\n",
    "backbone = AutoModelForTokenClassification.from_pretrained(MODEL_NAME, config=config_model)\n",
    "backbone.save_pretrained('model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15594/15594 [00:04<00:00, 3653.43it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 1663.35it/s]\n"
     ]
    }
   ],
   "source": [
    "# 데이터 로딩\n",
    "## 다운로드 (using Kaggle API)\n",
    "# !kaggle competitions download -q -c feedback-prize-2021\n",
    "# !unzip feedback-prize-2021.zip -d data\n",
    "\n",
    "## train.csv\n",
    "train_df = pd.read_csv('data/train.csv')\n",
    "\n",
    "## train\n",
    "train_ids, train_texts = [], []\n",
    "for f in tqdm(os.listdir('data/train')):\n",
    "    train_ids.append(f.replace('.txt', ''))\n",
    "    train_texts.append(open(f'data/train/{f}', 'r', encoding='utf8').read())\n",
    "train_text_df = pd.DataFrame({'id': train_ids, 'text': train_texts})\n",
    "\n",
    "## test\n",
    "test_ids, test_texts = [], []\n",
    "for f in tqdm(os.listdir('data/test')):\n",
    "    test_ids.append(f.replace('.txt', ''))\n",
    "    test_texts.append(open(f'data/test/{f}', 'r', encoding='utf8').read())\n",
    "test_text_df = pd.DataFrame({'id': test_ids, 'text': test_texts})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 가공\n",
    "# all_entities = []\n",
    "# for i, row in train_text_df.iterrows():\n",
    "#     if i%100 == 0: print(i, '', end='')\n",
    "#     total = len(row['text'].split())\n",
    "#     id = row['id']\n",
    "#     entities = ['O']*total\n",
    "#     for _, discource in train_df.query(\"id == @id\").iterrows():\n",
    "#         disc_type = discource['discourse_type']\n",
    "#         list_ix = [int(x) for x in discource['predictionstring'].split(' ')]\n",
    "#         entities[list_ix[0]] = f'B-{disc_type}'\n",
    "#         for j in list_ix[1:]:\n",
    "#             entities[j] = f'I-{disc_type}'\n",
    "#     all_entities.append(entities)\n",
    "# train_text_df['entities'] = all_entities\n",
    "# train_text_df.to_csv('tokens/train_NER.csv', index=False)\n",
    "\n",
    "train_text_df = pd.read_csv('tokens/train_NER.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 클래스 정의\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length, get_wids):\n",
    "        self.len = len(data)\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.get_wids = get_wids\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = self.data.text[index]\n",
    "        word_labels = eval(self.data.entities[index]) if not self.get_wids else None\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            text.split(),\n",
    "            is_split_into_words = True,\n",
    "            padding = 'max_length',\n",
    "            truncation = True,\n",
    "            max_length = self.max_length\n",
    "        )\n",
    "        word_ids = encoding.word_ids()\n",
    "        item = {key: torch.as_tensor(val) for key, val in encoding.items()}\n",
    "\n",
    "        if not self.get_wids:\n",
    "            label_ids = []\n",
    "            for word_idx in word_ids:\n",
    "                label_ids.append(LABELS_TO_IDS[word_labels[word_idx]] if word_idx is not None else -100)\n",
    "            item['labels'] = torch.as_tensor(label_ids)\n",
    "        else:\n",
    "            item['wids'] = torch.as_tensor([w if w is not None else -1 for w in word_ids])\n",
    "\n",
    "        return item\n",
    "                \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로더 생성\n",
    "IDS = train_df.id.unique()\n",
    "np.random.seed(42)\n",
    "train_idx = np.random.choice(np.arange(len(IDS)), int(0.9*len(IDS)), replace=False)\n",
    "valid_idx = np.setdiff1d(np.arange(len(IDS)), train_idx)\n",
    "np.random.seed(None)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('model')\n",
    "\n",
    "train_dataset = train_text_df.query('id in @IDS[@train_idx]')[['text', 'entities']].reset_index(drop=True)\n",
    "valid_dataset = train_text_df.query('id in @IDS[@valid_idx]')[['text', 'entities']].reset_index(drop=True)\n",
    "test_dataset = test_text_df.copy()\n",
    "\n",
    "training_set = CustomDataset(train_dataset, tokenizer, CONFIG['max_length'], False)\n",
    "validating_set = CustomDataset(valid_dataset, tokenizer, CONFIG['max_length'], True)\n",
    "testing_set = CustomDataset(test_dataset, tokenizer, CONFIG['max_length'], True)\n",
    "\n",
    "train_params = {\n",
    "    'batch_size': CONFIG['train_batch_size'],\n",
    "    'shuffle': True,\n",
    "    'num_workers': 2,\n",
    "    'pin_memory': True\n",
    "}\n",
    "\n",
    "valid_params = {\n",
    "    'batch_size': CONFIG['valid_batch_size'],\n",
    "    'shuffle': False,\n",
    "    'num_workers': 2,\n",
    "    'pin_memory': True\n",
    "}\n",
    "\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "validating_loader = DataLoader(validating_set, **valid_params)\n",
    "testing_loader = DataLoader(testing_set, **valid_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 설정\n",
    "config_model = AutoConfig.from_pretrained('model/config.json')\n",
    "model = AutoModelForTokenClassification.from_pretrained('model/pytorch_model.bin', config=config_model)\n",
    "model.to(CONFIG['device'])\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=CONFIG['learning_rates'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([  65, 2874,  762,  ...,    0,    0,    0]),\n",
       " 'attention_mask': tensor([1, 1, 1,  ..., 0, 0, 0]),\n",
       " 'labels': tensor([-100,    3,    4,  ..., -100, -100, -100])}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 학습\n",
    "for epoch in range(CONFIG['epochs']):\n",
    "\n",
    "    for g in optimizer.param_groups:\n",
    "        g['lr'] = CONFIG['learning_rates'][epoch]\n",
    "    lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "    tr_loss, tr_accuracy = 0, 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    model.train()\n",
    "    for _, batch in enumerate(training_loader):\n",
    "        ids = batch['input_ids'].to(CONFIG['device'], dtype=torch.long).unsqueeze(0)\n",
    "        mask = batch['attention_mask'].to(CONFIG['device'], dtype=torch.long).unsqueeze(0)\n",
    "        labels = batch['labels'].to(CONFIG['device'], dtype=torch.long).unsqueeze(0)\n",
    "        loss, tr_logits = model(input_ids=ids, attention_mask=mask, labels=labels, return_dict=False)\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_steps += 1\n",
    "        nb_tr_examples += labels.size(0)\n",
    "        flattened_targets = labels.view(-1)\n",
    "        active_logits = tr_logits.view(-1, model.num_labels)\n",
    "        flattened_predictions = torch.argmax(active_logits, axis=1)\n",
    "        active_accuracy = labels.view(-1) != -100\n",
    "        labels = torch.masked_select(flattened_targets, active_accuracy)\n",
    "        predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
    "        tr_accuracy += accuracy_score(labels.cpu().numpy(), predictions.cpu().numpy())\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=CONFIG['max_grad_norm'])\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "# torch.save(model.state_dict(), f'bigbird_v{VER}.pt')\n",
    "# model.load_state_dict(torch.load(f'bigbird_v{VER}.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(batch):\n",
    "                \n",
    "    # MOVE BATCH TO GPU AND INFER\n",
    "    ids = batch[\"input_ids\"].to(config['device'])\n",
    "    mask = batch[\"attention_mask\"].to(config['device'])\n",
    "    outputs = model(ids, attention_mask=mask, return_dict=False)\n",
    "    all_preds = torch.argmax(outputs[0], axis=-1).cpu().numpy() \n",
    "\n",
    "    # INTERATE THROUGH EACH TEXT AND GET PRED\n",
    "    predictions = []\n",
    "    for k,text_preds in enumerate(all_preds):\n",
    "        token_preds = [ids_to_labels[i] for i in text_preds]\n",
    "\n",
    "        prediction = []\n",
    "        word_ids = batch['wids'][k].numpy()  \n",
    "        previous_word_idx = -1\n",
    "        for idx,word_idx in enumerate(word_ids):                            \n",
    "            if word_idx == -1:\n",
    "                pass\n",
    "            elif word_idx != previous_word_idx:              \n",
    "                prediction.append(token_preds[idx])\n",
    "                previous_word_idx = word_idx\n",
    "        predictions.append(prediction)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(df=test_dataset, loader=testing_loader):\n",
    "    \n",
    "    # put model in training mode\n",
    "    model.eval()\n",
    "    \n",
    "    # GET WORD LABEL PREDICTIONS\n",
    "    y_pred2 = []\n",
    "    for batch in loader:\n",
    "        labels = inference(batch)\n",
    "        y_pred2.extend(labels)\n",
    "\n",
    "    final_preds2 = []\n",
    "    for i in range(len(df)):\n",
    "\n",
    "        idx = df.id.values[i]\n",
    "        #pred = [x.replace('B-','').replace('I-','') for x in y_pred2[i]]\n",
    "        pred = y_pred2[i] # Leave \"B\" and \"I\"\n",
    "        preds = []\n",
    "        j = 0\n",
    "        while j < len(pred):\n",
    "            cls = pred[j]\n",
    "            if cls == 'O': j += 1\n",
    "            else: cls = cls.replace('B','I') # spans start with B\n",
    "            end = j + 1\n",
    "            while end < len(pred) and pred[end] == cls:\n",
    "                end += 1\n",
    "            \n",
    "            if cls != 'O' and cls != '' and end - j > 7:\n",
    "                final_preds2.append((idx, cls.replace('I-',''),\n",
    "                                     ' '.join(map(str, list(range(j, end))))))\n",
    "        \n",
    "            j = end\n",
    "        \n",
    "    oof = pd.DataFrame(final_preds2)\n",
    "    oof.columns = ['id','class','predictionstring']\n",
    "\n",
    "    return oof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_overlap(row):\n",
    "    \"\"\"\n",
    "    Calculates the overlap between prediction and\n",
    "    ground truth and overlap percentages used for determining\n",
    "    true positives.\n",
    "    \"\"\"\n",
    "    set_pred = set(row.predictionstring_pred.split(' '))\n",
    "    set_gt = set(row.predictionstring_gt.split(' '))\n",
    "    # Length of each and intersection\n",
    "    len_gt = len(set_gt)\n",
    "    len_pred = len(set_pred)\n",
    "    inter = len(set_gt.intersection(set_pred))\n",
    "    overlap_1 = inter / len_gt\n",
    "    overlap_2 = inter/ len_pred\n",
    "    return [overlap_1, overlap_2]\n",
    "\n",
    "\n",
    "def score_feedback_comp(pred_df, gt_df):\n",
    "    \"\"\"\n",
    "    A function that scores for the kaggle\n",
    "        Student Writing Competition\n",
    "        \n",
    "    Uses the steps in the evaluation page here:\n",
    "        https://www.kaggle.com/c/feedback-prize-2021/overview/evaluation\n",
    "    \"\"\"\n",
    "    gt_df = gt_df[['id','discourse_type','predictionstring']] \\\n",
    "        .reset_index(drop=True).copy()\n",
    "    pred_df = pred_df[['id','class','predictionstring']] \\\n",
    "        .reset_index(drop=True).copy()\n",
    "    pred_df['pred_id'] = pred_df.index\n",
    "    gt_df['gt_id'] = gt_df.index\n",
    "    # Step 1. all ground truths and predictions for a given class are compared.\n",
    "    joined = pred_df.merge(gt_df,\n",
    "                           left_on=['id','class'],\n",
    "                           right_on=['id','discourse_type'],\n",
    "                           how='outer',\n",
    "                           suffixes=('_pred','_gt')\n",
    "                          )\n",
    "    joined['predictionstring_gt'] = joined['predictionstring_gt'].fillna(' ')\n",
    "    joined['predictionstring_pred'] = joined['predictionstring_pred'].fillna(' ')\n",
    "\n",
    "    joined['overlaps'] = joined.apply(calc_overlap, axis=1)\n",
    "\n",
    "    # 2. If the overlap between the ground truth and prediction is >= 0.5, \n",
    "    # and the overlap between the prediction and the ground truth >= 0.5,\n",
    "    # the prediction is a match and considered a true positive.\n",
    "    # If multiple matches exist, the match with the highest pair of overlaps is taken.\n",
    "    joined['overlap1'] = joined['overlaps'].apply(lambda x: eval(str(x))[0])\n",
    "    joined['overlap2'] = joined['overlaps'].apply(lambda x: eval(str(x))[1])\n",
    "\n",
    "\n",
    "    joined['potential_TP'] = (joined['overlap1'] >= 0.5) & (joined['overlap2'] >= 0.5)\n",
    "    joined['max_overlap'] = joined[['overlap1','overlap2']].max(axis=1)\n",
    "    tp_pred_ids = joined.query('potential_TP') \\\n",
    "        .sort_values('max_overlap', ascending=False) \\\n",
    "        .groupby(['id','predictionstring_gt']).first()['pred_id'].values\n",
    "\n",
    "    # 3. Any unmatched ground truths are false negatives\n",
    "    # and any unmatched predictions are false positives.\n",
    "    fp_pred_ids = [p for p in joined['pred_id'].unique() if p not in tp_pred_ids]\n",
    "\n",
    "    matched_gt_ids = joined.query('potential_TP')['gt_id'].unique()\n",
    "    unmatched_gt_ids = [c for c in joined['gt_id'].unique() if c not in matched_gt_ids]\n",
    "\n",
    "    # Get numbers of each type\n",
    "    TP = len(tp_pred_ids)\n",
    "    FP = len(fp_pred_ids)\n",
    "    FN = len(unmatched_gt_ids)\n",
    "    #calc microf1\n",
    "    my_f1_score = TP / (TP + 0.5*(FP+FN))\n",
    "    return my_f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if COMPUTE_VAL_SCORE: # note this doesn't run during submit\n",
    "    # VALID TARGETS\n",
    "    valid = train_df.loc[train_df['id'].isin(IDS[valid_idx])]\n",
    "\n",
    "    # OOF PREDICTIONS\n",
    "    oof = get_predictions(test_dataset, testing_loader)\n",
    "\n",
    "    # COMPUTE F1 SCORE\n",
    "    f1s = []\n",
    "    CLASSES = oof['class'].unique()\n",
    "    print()\n",
    "    for c in CLASSES:\n",
    "        pred_df = oof.loc[oof['class']==c].copy()\n",
    "        gt_df = valid.loc[valid['discourse_type']==c].copy()\n",
    "        f1 = score_feedback_comp(pred_df, gt_df)\n",
    "        print(c,f1)\n",
    "        f1s.append(f1)\n",
    "    print()\n",
    "    print('Overall',np.mean(f1s))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = get_predictions(test_texts, test_texts_loader)\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4e0b39fdc4f2df2992ef135d097972ec225e019845a073717c5b1bc8df292c8a"
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
