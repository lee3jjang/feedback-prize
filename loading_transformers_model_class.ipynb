{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup \n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer \n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/Reviews.csv', nrows=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11178</th>\n",
       "      <td>If you have serious food allergies or gluten i...</td>\n",
       "      <td>Yummy, little crunchy cookies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22128</th>\n",
       "      <td>I bought this for my 6 month old lab puppy. He...</td>\n",
       "      <td>Not for a large puppy or chewer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76138</th>\n",
       "      <td>These natural chewy candies have a wonderful u...</td>\n",
       "      <td>Ginger Chews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13687</th>\n",
       "      <td>Our black lab devoured this thing in 10 minute...</td>\n",
       "      <td>just an expensive dog treat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82931</th>\n",
       "      <td>I've been a San Francisco Bay coffee fan for y...</td>\n",
       "      <td>WONDERFUL!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798</th>\n",
       "      <td>Of course, we all know how delicious Ghirardel...</td>\n",
       "      <td>Great chocolate...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37574</th>\n",
       "      <td>I love the strawberry flavor of these.  I boug...</td>\n",
       "      <td>Love these!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44014</th>\n",
       "      <td>My boys love love love this brand and this par...</td>\n",
       "      <td>BOYS LOVEEE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4787</th>\n",
       "      <td>This food is a vast improvement over the store...</td>\n",
       "      <td>Blows away store-bought</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12963</th>\n",
       "      <td>You will become addicted to it once you eat it...</td>\n",
       "      <td>The best kind of crackers on earth</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Text  \\\n",
       "11178  If you have serious food allergies or gluten i...   \n",
       "22128  I bought this for my 6 month old lab puppy. He...   \n",
       "76138  These natural chewy candies have a wonderful u...   \n",
       "13687  Our black lab devoured this thing in 10 minute...   \n",
       "82931  I've been a San Francisco Bay coffee fan for y...   \n",
       "798    Of course, we all know how delicious Ghirardel...   \n",
       "37574  I love the strawberry flavor of these.  I boug...   \n",
       "44014  My boys love love love this brand and this par...   \n",
       "4787   This food is a vast improvement over the store...   \n",
       "12963  You will become addicted to it once you eat it...   \n",
       "\n",
       "                                  Summary  \n",
       "11178       Yummy, little crunchy cookies  \n",
       "22128     Not for a large puppy or chewer  \n",
       "76138                        Ginger Chews  \n",
       "13687         just an expensive dog treat  \n",
       "82931                          WONDERFUL!  \n",
       "798                    Great chocolate...  \n",
       "37574                         Love these!  \n",
       "44014                         BOYS LOVEEE  \n",
       "4787              Blows away store-bought  \n",
       "12963  The best kind of crackers on earth  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data[['Text', 'Summary']]\n",
    "data.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72348"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Text'].nunique()\n",
    "data['Summary'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "      <td>Not as Advertised</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "      <td>Cough Medicine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "      <td>Great taffy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>I just love it and will buy another box when I...</td>\n",
       "      <td>yummy!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>My late father in law used to have a rating sy...</td>\n",
       "      <td>Tastes like More!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>This is my favorite brand of Korean ramen. It ...</td>\n",
       "      <td>Great ramen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>I do like these noodles although, to say they ...</td>\n",
       "      <td>Spicy!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>I love this noodle and have it once or twice a...</td>\n",
       "      <td>This spicy noodle cures my cold, upset stomach...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>88425 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Text  \\\n",
       "0      I have bought several of the Vitality canned d...   \n",
       "1      Product arrived labeled as Jumbo Salted Peanut...   \n",
       "2      This is a confection that has been around a fe...   \n",
       "3      If you are looking for the secret ingredient i...   \n",
       "4      Great taffy at a great price.  There was a wid...   \n",
       "...                                                  ...   \n",
       "99995  I just love it and will buy another box when I...   \n",
       "99996  My late father in law used to have a rating sy...   \n",
       "99997  This is my favorite brand of Korean ramen. It ...   \n",
       "99998  I do like these noodles although, to say they ...   \n",
       "99999  I love this noodle and have it once or twice a...   \n",
       "\n",
       "                                                 Summary  \n",
       "0                                  Good Quality Dog Food  \n",
       "1                                      Not as Advertised  \n",
       "2                                  \"Delight\" says it all  \n",
       "3                                         Cough Medicine  \n",
       "4                                            Great taffy  \n",
       "...                                                  ...  \n",
       "99995                                             yummy!  \n",
       "99996                                  Tastes like More!  \n",
       "99997                                        Great ramen  \n",
       "99998                                            Spicy!!  \n",
       "99999  This spicy noodle cures my cold, upset stomach...  \n",
       "\n",
       "[88425 rows x 2 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.drop_duplicates(subset=['Text']).dropna(axis=0)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\", \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\", \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\", \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\"}\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence: str, remove_stopwords: bool = True):\n",
    "    sentence = sentence.lower()\n",
    "    sentence = BeautifulSoup(sentence, 'lxml').text\n",
    "    sentence = re.sub(r'\\([^)]*\\)', '', sentence)\n",
    "    sentence = re.sub('\"', '', sentence)\n",
    "    sentence = ' '.join([contractions[t] if t in contractions else t for t in sentence.split(' ')])\n",
    "    sentence = re.sub(r\"'s\\b\", '', sentence)\n",
    "    sentence = re.sub(r\"'[^a-zA-Z]\", '', sentence)\n",
    "    sentence = re.sub(r'[m]{2,}', 'mm', sentence)\n",
    "    # sentence.split()\n",
    "    if remove_stopwords:\n",
    "        tokens = ' '.join(word for word in sentence.split() if not word in stop_words if len(word) > 1)\n",
    "    else:\n",
    "        tokens = ' '.join(word for word in sentence.split() if len(word) > 1)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text = []\n",
    "for s in data['Text']:\n",
    "    clean_text.append(preprocess_sentence(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:337: MarkupResemblesLocatorWarning: \"...\" looks like a directory name, not markup. You may want to open a file found in this directory and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:431: MarkupResemblesLocatorWarning: \"http://www.amazon.com/gp/product/b007i7yygy/ref=cm_cr_rev_prod_title\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "clean_summary = []\n",
    "for s in data['Summary']:\n",
    "    clean_summary.append(preprocess_sentence(s, False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Text'] = clean_text\n",
    "data['Summary'] = clean_summary\n",
    "data = data.replace('', np.nan)\n",
    "data = data.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.loc[data['Text'].apply(lambda x: len(x.split()) <= 50)]\n",
    "data = data.loc[data['Summary'].apply(lambda x: len(x.split()) <= 8)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['decoder_input'] = data['Summary'].apply(lambda x: 'sostoken ' + x)\n",
    "data['decoder_target'] = data['Summary'].apply(lambda x: x + ' eostoken')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = np.array(data['Text'])\n",
    "decoder_input = np.array(data['decoder_input'])\n",
    "decoder_target = np.array(data['decoder_target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.arange(encoder_input.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "encoder_input = encoder_input[indices]\n",
    "decoder_input = decoder_input[indices]\n",
    "decoder_target = decoder_target[indices]\n",
    "n_of_val = int(len(encoder_input)*0.2)\n",
    "\n",
    "encoder_input_train = encoder_input[:-n_of_val]\n",
    "decoder_input_train = decoder_input[:-n_of_val]\n",
    "decoder_target_train = decoder_target[:-n_of_val]\n",
    "\n",
    "encoder_input_test = encoder_input[-n_of_val:]\n",
    "decoder_input_test = decoder_input[-n_of_val:]\n",
    "decoder_target_test = decoder_target[-n_of_val:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_tokenizer = Tokenizer(num_words=8000)\n",
    "src_tokenizer.fit_on_texts(encoder_input_train)\n",
    "\n",
    "encoder_input_int_train = src_tokenizer.texts_to_sequences(encoder_input_train)\n",
    "encoder_input_int_test = src_tokenizer.texts_to_sequences(encoder_input_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "tar_tokenizer = Tokenizer(num_words=2000)\n",
    "tar_tokenizer.fit_on_texts(decoder_input_train)\n",
    "tar_tokenizer.fit_on_texts(decoder_target_train)\n",
    "\n",
    "decoder_input_int_train = tar_tokenizer.texts_to_sequences(decoder_input_train)\n",
    "decoder_target_int_train = tar_tokenizer.texts_to_sequences(decoder_target_train)\n",
    "decoder_input_int_test = tar_tokenizer.texts_to_sequences(decoder_input_test)\n",
    "decoder_target_int_test = tar_tokenizer.texts_to_sequences(decoder_target_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\_asarray.py:102: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    }
   ],
   "source": [
    "drop_train = [index for index, sentence in enumerate(decoder_input_int_train) if len(sentence)==1]\n",
    "drop_test = [index for index, sentence in enumerate(decoder_input_int_test) if len(sentence)==1]\n",
    "\n",
    "encoder_input_int_droped_train = np.delete(encoder_input_int_train, drop_train, axis=0)\n",
    "decoder_input_int_droped_train = np.delete(decoder_input_int_train, drop_train, axis=0)\n",
    "decoder_target_int_droped_train = np.delete(decoder_target_int_train, drop_train, axis=0)\n",
    "\n",
    "encoder_input_int_droped_test = np.delete(encoder_input_int_test, drop_test, axis=0)\n",
    "decoder_input_int_droped_test = np.delete(decoder_input_int_test, drop_test, axis=0)\n",
    "decoder_target_int_droped_test = np.delete(decoder_target_int_test, drop_test, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_train = pad_sequences(encoder_input_int_droped_train, maxlen=50, padding='post')\n",
    "encoder_input_test = pad_sequences(encoder_input_int_droped_test, maxlen=50, padding='post')\n",
    "decoder_input_train = pad_sequences(decoder_input_int_droped_train, maxlen=8, padding='post')\n",
    "decoder_input_test = pad_sequences(encoder_input_int_droped_test, maxlen=8, padding='post')\n",
    "decoder_target_train = pad_sequences(decoder_target_int_droped_train, maxlen=8, padding='post')\n",
    "decoder_target_test = pad_sequences(decoder_target_int_droped_test, maxlen=8, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   3,  118,    2, ...,    0,    0,    0],\n",
       "       [ 250,  198,    2, ...,    0,    0,    0],\n",
       "       [  81,    6,  568, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [ 314,   52,  631, ...,    0,    0,    0],\n",
       "       [   6,   40,    2, ...,    0,    0,    0],\n",
       "       [ 179,   46, 1274, ...,    2,    0,    0]])"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_target_test"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
